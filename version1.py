# -*- coding: utf-8 -*-
"""version1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YndDnHL6fyR2__7gCiAVC87qDniFD0zO
"""

#this part load a data into google colab
from google.colab import drive
import pandas as pd
from pandas import DataFrame
drive.mount('/content/drive', force_remount="True")
#Read dataset
path = '/content/drive/My Drive/gephi/titlerepubblica.csv'
pathnomi = '/content/drive/My Drive/gephi/nomi_italiani.txt' #https://gist.github.com/pdesterlich/2562329#file-nomi_italiani-txt
pathcognomi = '/content/drive/My Drive/gephi/cognomi.txt' #https://github.com/PaoloSarti/lista_cognomi_italiani
pathstati = '/content/drive/My Drive/gephi/stati.txt' #http://www.salute.gov.it/imgs/C_17_pubblicazioni_1055_ulterioriallegati_ulterioreallegato_0_alleg.txt
pathcomuni = '/content/drive/My Drive/gephi/comuni.csv'
#-----------------------------------------------------------------#
import os
#NOMI ITALIANI
var_nomi_italiani = open(pathnomi,'r').read().split('\n')
for i in range (0, len(var_nomi_italiani)):
  check = var_nomi_italiani[i]
  var_nomi_italiani[i] = check.capitalize()
print("Nomi italiani")
print(var_nomi_italiani)
print("")
#COGNOMI ITALIANI
var_cognomi = open(pathcognomi,'r').read().split('\n')
print("Cognomi italiani")
print(var_cognomi)
print("")
#STATI ITALIANI
var_stati = open(pathstati,'r').read().split('\t')
var_stati.pop(0)
var_stati.pop(0)
var_stati.pop(0)
for i in range(0,len(var_stati)):
  if i % 2 != 0:
    var_stati[i]=""
var_stati_clean=[]
[var_stati_clean.append(item) for item in var_stati if item not in var_stati_clean]
var_stati_clean.pop(1)
print("Stati italiani puliti")
print(var_stati_clean)
print("")

#COMUNI ITALINI
df1 = pd.read_csv('/content/drive/My Drive/gephi/comuni.csv', encoding = "ISO-8859-1")
list_comuni = df1.values.tolist()
var_comuni = []
for elem in list_comuni:
    var_comuni.extend(elem)
print("Tutti i comuni italiani")
print(var_comuni)
print("")

#NOMI PROPRI ITALIANI
lista_ita = var_cognomi + var_nomi_italiani + var_comuni + var_stati_clean
lista_ita.append("Know") #ulteriore pulizia data dai titoli 
print(f'In tutto i nomi prori in italiano sono -->  {len(lista_ita)}')

"""Funzione nella quale andiamo a caricare il file preso da google drive (altrimenti andrebbe ricaricato ogni volta) e si occupa di eseguire la pulizia del testo."""

import nltk
import pandas as pd
from nltk.stem import PorterStemmer #https://www.nltk.org/howto/stem.html
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.snowball import ItalianStemmer
import re

nltk.download('punkt')
#download the stopwords
nltk.download('stopwords')
#load the italian stopwords
stops = set(stopwords.words("italian"))
#https://www.datacamp.com/community/tutorials/stemming-lemmatization-python?utm_source=adwords_ppc&utm_campaignid=9942305733&utm_adgroupid=100189364546&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=332602034352&utm_targetid=aud-392016246653:dsa-929501846124&utm_loc_interest_ms=&utm_loc_physical_ms=1008634&gclid=CjwKCAjw5vz2BRAtEiwAbcVIL0VLYkzQfiFr4t0Y9FlyRYPY2eKEm50uOEwo5jaelfUUhUJ-vGY0mBoCqjwQAvD_BwE
# quello che si occupa di togliere le ultime lettere per rendere uguali le parole tra loro esempio play = played, playing, plays
#stemming = PorterStemmer()
stemming = SnowballStemmer("italian", ignore_stopwords=False)

#--------------------------------------------------------------------------------#

def apply_cleaning_function_to_list(X):
    cleaned_X = []
    for element in X:
        cleaned_X.append(clean_text(element))
    return cleaned_X

def clean_text(raw_text):

  text = raw_text #NON INDISPENSABILE

  #controllo delle lettere maiuscole dopo i segni di punteggiatura italiani
  #check "capital letters" after Italian punctuation marks
  lista = list(text)
  for i in range(0,len(lista)):
    if(lista[i]=='"' and i+1 < len(lista) or lista[i]=='“' and i+1 < len(lista) or lista[i]=="'" and i+1 < len(lista)):
      if lista[i+1] not in lista_ita:
        lista[i+1] = lista[i+1].lower()

    if(lista[i]=='.'  and i+1 < len(lista)):
      if(lista[i+1]==' ' and i+2 < len(lista)):
        if lista[i+2] not in lista_ita:
          lista[i+2] = lista[i+2].lower()
    
    if(lista[i]=='?'  and i+1 < len(lista)):
      if(lista[i+1]==' ' and i+2 < len(lista)):
        if lista[i+2] not in lista_ita:
          lista[i+2] = lista[i+2].lower()
    
    if(lista[i]=='!'  and i+1 < len(lista)):
      if(lista[i+1]==' ' and i+2 < len(lista)):
        if lista[i+2] not in lista_ita :
          lista[i+2] = lista[i+2].lower()
          
  text = "".join(lista)

  # Tokenize with the italian words
  tokens = nltk.word_tokenize(text, language='italian')
  #  Convert to lower case the first word into EVERY TITLE
  if tokens[0] not in lista_ita:
      tokens[0] = tokens[0].lower()

  # exclude the word "nan" because generate an errors
  token_words = [w for w in tokens if w != "nan"] 
  
  text_nostopword = token_words
  #check capital words
  for i in range (0,len(text_nostopword)):
    check = text_nostopword[i]
    #Python string method islower() checks whether all the case-based characters (letters) of the string are lowercase.
    # "THIS is string example....wow!!!" --> FALSE 
    # "this is string example....wow!!!" --> TRUE 
    check3 = ""
    if(check.lower() not in stops):
      if check.islower() :
        text_nostopword[i] = stemming.stem(check)
      elif (text_nostopword[(i)]).isalpha():
        if i+1 < len(text_nostopword):
          if not((text_nostopword[(i+1)]).isalpha()):
            continue
          check2 = text_nostopword[(i+1)]
          if check2.istitle():
            if check2.lower() not in stops:
              text_nostopword[i] = check + check2
              text_nostopword[i+1]= ""
          if i+2 < len(text_nostopword):
            if not((text_nostopword[(i+2)]).isalpha()):
              continue
            check3 = text_nostopword[(i+2)]
          #The istitle() method returns True if all words in a text start with a upper case letter, AND the rest of the word are lower case letters, otherwise False.
            if check2.istitle() and check2.lower() not in stops and check3.istitle() and check3.lower() not in stops:
              #unisci le due parole
              text_nostopword[i] = check + check2 + check3
              text_nostopword[i+1]= ""
              text_nostopword[i+2]= ""

  # Remove the numbers
  # take words made characters --> ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
  # isalpha() --> https://www.geeksforgeeks.org/python-string-isalpha-application/#:~:text=In%20Python%2C%20isalpha()%20is,%2C%20It%20returns%20%E2%80%9CFalse%E2%80%9D.
  text_nostopword = [w for w in text_nostopword if  w.isalpha() ] 
  # Remove stop words
  text_nostopwords = [w for w in text_nostopword if not w.lower() in stops and len(w)>2 ]

 
  #Rejoin words
  joined_words = ( " ".join(text_nostopwords))

  #resub the similar word 
  joined_words = re.sub(r"coronavirus", "Coronavirus", joined_words)
  joined_words = re.sub(r"covid", "Coronavirus", joined_words)
  joined_words = re.sub(r"Covid", "Coronavirus", joined_words)
  # è possibile che l'articolo non abbia titolo quindi viene salvato come no titolo che dopo la pulizia 
  # viene trasformato in titol
  # è stato deciso di non considerarlo 
  joined_words = re.sub(r"titol", "", joined_words)
  
  testopulito = joined_words #NON INDISPENSABILE
  
  return testopulito
#---------------------------------------------------------------------------------#

# Load data 
imdb = pd.read_csv(path)

#imdb = imdb.head(15) #serve per prendere solo le righe indicate 

# Get text to clean
text_to_clean = list(imdb['title'])


for i in range (0, len(text_to_clean)):
  text_to_clean[i] = str(text_to_clean[i])
# Clean text
cleaned_text = apply_cleaning_function_to_list(text_to_clean)

# Show example
print ('Original text:',text_to_clean[5])
print ('\nCleaned text:', cleaned_text[5])

# Add cleaned data back into DataFrame
imdb['title'] = cleaned_text
imdb.to_csv('imdb.csv', index=False) #True per avere id prima della frase

"""Funzione per creare il csv con tutti i nodi del grafo e la lista con i nodi"""

import csv
with open('nodes_table.csv', 'w',newline='') as f:
  with open('imdb.csv') as csv_file:
    nodi = []
    csv_reader = csv.reader(csv_file, delimiter=' ')
    thewriter = csv.writer(f) #scrittura
    thewriter.writerow(['Id','Label']) #scrittura
    for row in csv_reader:
        for i in range (0,len(row)):
          if row[i] not in nodi and len(row[i])>2:
            nodi.append(row[i])
            #print(f'{row[i]} \n')
    nodi.sort()
    print(nodi)
    for i in range(0,len(nodi)):
      thewriter.writerow([i,nodi[i]])
print(len(nodi))

"""Funzione che va a creare un csv con tutti gli archi del grafo."""

# FUNGE NON TOCCARE MANCO SE MORI
import csv
with open('edges_table.csv', 'w',newline='') as f:
  with open('imdb.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=' ')
    lista_edge =[]
    line_count = 0
    thewriter = csv.writer(f) #scrittura
    thewriter.writerow(['Source','Target']) #scrittura
    for row in csv_reader:
        if line_count == 0:
            print(f'Column names are {", ".join(row)}')
            line_count += 1
        else:
            for i in range (0,len(row)):
              for c in range (i,len(row)):
                if c != i  and len(row[i])>2 and len (row[c])>2:
                  thewriter.writerow([nodi.index(row[i]),nodi.index(row[c])])
                  edge1 = [nodi.index(row[i]),nodi.index(row[c])]
                  lista_edge.append(edge1)
            line_count += 1
    print(f'Processed {line_count} lines.')
    print(lista_edge)
    print("GLI ARCHI TOTALI SONO : ",len(lista_edge))

"""Codice che crea una lista di archi univoci chiamata lista_sigle"""

import progressbar
bar = progressbar.ProgressBar(maxval=len(lista_edge), \
    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])

lista_single = []
bar.start()
for i in range(0,len(lista_edge)):

  bar.update(i+1)

  sourcei = lista_edge[i][0]
  targeti = lista_edge[i][1]
  inverso = [targeti, sourcei]

  if lista_edge[i] not in lista_single and inverso not in lista_single:
    lista_single.append(lista_edge[i])
  
bar.finish()
print(lista_single)
print("GLI ARCHI UNICI SONO : ",len(lista_single))

"""Parte di codice che crea una a lista di pesi andando a lavorare con la lista_single. <br>
Perciò sappiamo che il peso iesimo della lista_pesata sarà il peso dell'arco iesimo della lista_sigle. <br> <br>
esempio peso --> lista_pesata[i] sarà il peso dell'arco --> lista_single[i]
"""

import progressbar
bar = progressbar.ProgressBar(maxval=len(lista_single), \
    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])

lista_pesata = []
bar.start()
for i in range(0,len(lista_single)):
  
  bar.update(i+1)

  sourcei = lista_single[i][0]
  targeti = lista_single[i][1]
  inverso = [targeti, sourcei]

  peso1 = lista_edge.count(lista_single[i])
  peso2 = lista_edge.count(inverso)

  peso = peso1 + peso2
  lista_pesata.append(peso)

bar.finish()
print(lista_pesata)
print("GLI ARCHI PESATI SONO : ",len(lista_pesata))

"""Vado a scrivere su edges_table.csv i risultati ottenuti per poterli caricare su Ghepi"""

with open('edges_tableWeight.csv', 'w',newline='') as f:

  thewriter = csv.writer(f) #scrittura
  thewriter.writerow(['Source','Target','Weight']) #scrittura

  for i in range(0,len(lista_pesata)):
    thewriter.writerow([lista_single[i][0],lista_single[i][1],lista_pesata[i]])

"""La seguente funzione di "servizio" serve per ricercare all'interno della lista nodi se è presente una data stringa, che poi non sarebbe altro che il nodo stesso."""

nodoricerca = input('inserisci il nome del nodo da cercare --> ')
nodoricerca = nodoricerca.strip() #rimuove gli spazzi bianchi all'inizio e alla fine della stringa
if nodoricerca in nodi:
  print('presente')
else:
  print('assente')